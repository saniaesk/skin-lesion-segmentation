{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "running-tragedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "from segformer import *\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class Cross_Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, key_channels, value_channels, height, width, head_count=1):\n",
    "        super().__init__()\n",
    "        self.key_channels = key_channels\n",
    "        self.head_count = head_count\n",
    "        self.value_channels = value_channels\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "        self.reprojection = nn.Conv2d(value_channels, 2*value_channels, 1)\n",
    "        self.norm = nn.LayerNorm(2*value_channels)\n",
    "    #   self.attend         = nn.Softmax(dim = -1)\n",
    "\n",
    "    # x2 should be higher-level representation than x1\n",
    "    def forward(self, x1, x2):\n",
    "        B, N, D = x1.size()  # (Batch, Tokens, Embedding dim)\n",
    "\n",
    "        # Re-arrange into a (Batch, Embedding dim, Tokens)\n",
    "        keys = x2.transpose(1, 2)\n",
    "        queries = x2.transpose(1, 2)\n",
    "        values = x1.transpose(1, 2)\n",
    "        head_key_channels = self.key_channels // self.head_count\n",
    "        head_value_channels = self.value_channels // self.head_count\n",
    "\n",
    "        attended_values = []\n",
    "        for i in range(self.head_count):\n",
    "            key = F.softmax(\n",
    "                keys[:, i * head_key_channels: (i + 1) * head_key_channels, :], dim=2)\n",
    "            query = F.softmax(\n",
    "                queries[:, i * head_key_channels: (i + 1) * head_key_channels, :], dim=1)\n",
    "            value = values[:, i *\n",
    "                           head_value_channels: (i + 1) * head_value_channels, :]\n",
    "            context = key @ value.transpose(1, 2)  # dk*dv\n",
    "            attended_value = (context.transpose(1, 2) @ query)  # n*dv\n",
    "            attended_values.append(attended_value)\n",
    "\n",
    "        aggregated_values = torch.cat(attended_values, dim=1).reshape(\n",
    "            B, D, self.height, self.width)\n",
    "        reprojected_value = self.reprojection(\n",
    "            aggregated_values).reshape(B, 2*D, N).permute(0, 2, 1)\n",
    "        reprojected_value = self.norm(reprojected_value)\n",
    "\n",
    "        return reprojected_value\n",
    "\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "        Input ->    x1:[B, N, D] - N = H*W\n",
    "                    x2:[B, N, D]\n",
    "        Output -> y:[B, N, D]\n",
    "        D is half the size of the concatenated input (x1 from a lower level and x2 from the skip connection)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, key_dim, value_dim, height, width, head_count=1, token_mlp='mix'):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(in_dim)\n",
    "        self.H = height\n",
    "        self.W = width\n",
    "        self.attn = Cross_Attention(key_dim, value_dim, height, width, head_count=head_count)\n",
    "        self.norm2 = nn.LayerNorm((in_dim*2))\n",
    "        if token_mlp=='mix':\n",
    "            self.mlp = MixFFN((in_dim*2), int(in_dim*4))  \n",
    "        elif token_mlp=='mix_skip':\n",
    "            self.mlp = MixFFN_skip((in_dim*2), int(in_dim*4)) \n",
    "        else:\n",
    "            self.mlp = MLP_FFN((in_dim*2), int(in_dim*4))\n",
    "\n",
    "    def forward(self, x1: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:\n",
    "        norm_1 = self.norm1(x1)\n",
    "        norm_2 = self.norm1(x2)\n",
    "        \n",
    "        attn = self.attn(norm_1, norm_2)\n",
    "        #attn = Rearrange('b (h w) d -> b h w d', h=self.H, w=self.W)(attn)\n",
    "        \n",
    "        #residual1 = Rearrange('b (h w) d -> b h w d', h=self.H, w=self.W)(x1)\n",
    "        #residual2 = Rearrange('b (h w) d -> b h w d', h=self.H, w=self.W)(x2)\n",
    "        residual = torch.cat([x1, x2], dim=2)\n",
    "        tx = residual + attn\n",
    "        mx = tx + self.mlp(self.norm2(tx), self.H, self.W)\n",
    "        return mx\n",
    "\n",
    "class EfficientAttention(nn.Module):\n",
    "    \"\"\"\n",
    "        input  -> x:[B, D, H, W]\n",
    "        output ->   [B, D, H, W]\n",
    "    \n",
    "        in_channels:    int -> Embedding Dimension \n",
    "        key_channels:   int -> Key Embedding Dimension,   Best: (in_channels)\n",
    "        value_channels: int -> Value Embedding Dimension, Best: (in_channels or in_channels//2) \n",
    "        head_count:     int -> It divides the embedding dimension by the head_count and process each part individually\n",
    "        \n",
    "        Conv2D # of Params:  ((k_h * k_w * C_in) + 1) * C_out)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, key_channels, value_channels, head_count=1, flag_all = False):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.key_channels = key_channels\n",
    "        self.head_count = head_count\n",
    "        self.value_channels = value_channels\n",
    "        self.flag_all = flag_all\n",
    "        self.keys = nn.Conv2d(in_channels, key_channels, 1) \n",
    "        self.queries = nn.Conv2d(in_channels, key_channels, 1)\n",
    "        self.values = nn.Conv2d(in_channels, value_channels, 1)\n",
    "        self.reprojection = nn.Conv2d(value_channels, in_channels, 1)\n",
    "        self.flag_all = flag_all\n",
    "\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        n, _, h, w = input_.size()\n",
    "        \n",
    "        keys = self.keys(input_).reshape((n, self.key_channels, h * w))\n",
    "        queries = self.queries(input_).reshape(n, self.key_channels, h * w)\n",
    "        values = self.values(input_).reshape((n, self.value_channels, h * w))\n",
    "        \n",
    "        head_key_channels = self.key_channels // self.head_count\n",
    "        head_value_channels = self.value_channels // self.head_count\n",
    "        \n",
    "        attended_values = []\n",
    "        for i in range(self.head_count):\n",
    "            key = F.softmax(keys[\n",
    "                :,\n",
    "                i * head_key_channels: (i + 1) * head_key_channels,\n",
    "                :\n",
    "            ], dim=2)\n",
    "            \n",
    "            query = F.softmax(queries[\n",
    "                :,\n",
    "                i * head_key_channels: (i + 1) * head_key_channels,\n",
    "                :\n",
    "            ], dim=1)\n",
    "                        \n",
    "            value = values[\n",
    "                :,\n",
    "                i * head_value_channels: (i + 1) * head_value_channels,\n",
    "                :\n",
    "            ]            \n",
    "            \n",
    "            context = key @ value.transpose(1, 2) # dk*dv\n",
    "            attended_value = (context.transpose(1, 2) @ query).reshape(n, head_value_channels, h, w) # n*dv            \n",
    "            attended_values.append(attended_value)\n",
    "                \n",
    "        aggregated_values = torch.cat(attended_values, dim=1)\n",
    "        attention = self.reprojection(aggregated_values)\n",
    "        \n",
    "        if self.flag_all:\n",
    "            return context, query\n",
    "        else: \n",
    "            return attention\n",
    "  \n",
    "class ChannelAttention(nn.Module):\n",
    "    \"\"\"\n",
    "        Input -> x: [B, N, C]\n",
    "        Output -> [B, N, C]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0, proj_drop=0):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" x: [B, N, C]\n",
    "        \"\"\"\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        q = q.transpose(-2, -1)\n",
    "        k = k.transpose(-2, -1)\n",
    "        v = v.transpose(-2, -1)\n",
    "\n",
    "        q = F.normalize(q, dim=-1)\n",
    "        k = F.normalize(k, dim=-1)\n",
    "    \n",
    "        attn = (q @ k.transpose(-2, -1)) * self.temperature\n",
    "        # -------------------\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).permute(0, 3, 1, 2).reshape(B, N, C)\n",
    "        # ------------------\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class EfficientTransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "        Input  -> x (Size: (b, (H*W), d)), H, W\n",
    "        Output -> (b, (H*W), d)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, key_dim, value_dim, head_count=1, token_mlp='mix'):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(in_dim)\n",
    "        self.attn = EfficientAttention(in_channels=in_dim, key_channels=key_dim,\n",
    "                                       value_channels=value_dim, head_count=1)\n",
    "        self.norm2 = nn.LayerNorm(in_dim)\n",
    "        self.norm3 = nn.LayerNorm(in_dim)\n",
    "        #self.channel_attn = ChannelAttention(in_dim)\n",
    "        #self.norm4 = nn.LayerNorm(in_dim)\n",
    "        # add channel attention here\n",
    "        if token_mlp=='mix':\n",
    "            self.mlp1 = MixFFN(in_dim, int(in_dim*4))\n",
    "            self.mlp2 = MixFFN(in_dim, int(in_dim*4))\n",
    "        elif token_mlp=='mix_skip':\n",
    "            self.mlp1 = MixFFN_skip(in_dim, int(in_dim*4))\n",
    "            self.mlp2 = MixFFN_skip(in_dim, int(in_dim*4))\n",
    "        else:\n",
    "            self.mlp1 = MLP_FFN(in_dim, int(in_dim*4))\n",
    "            self.mlp2 = MLP_FFN(in_dim, int(in_dim*4))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, H, W) -> torch.Tensor:\n",
    "        # dual attention structure like it is used in the davit\n",
    "        norm1 = self.norm1(x)\n",
    "        norm1 = Rearrange('b (h w) d -> b d h w', h=H, w=W)(norm1)\n",
    "        \n",
    "        attn = self.attn(norm1)\n",
    "        attn = Rearrange('b d h w -> b (h w) d')(attn)\n",
    "\n",
    "        add1 = x + attn\n",
    "        norm2 = self.norm2(add1)\n",
    "        mlp1 = self.mlp1(norm2, H, W)\n",
    "\n",
    "        add2 = add1 + mlp1\n",
    "        norm3 = self.norm3(add2)\n",
    "        #channel_attn = self.channel_attn(norm3)\n",
    "\n",
    "        #add3 = add2 + channel_attn\n",
    "        #norm4 = self.norm4(add3)\n",
    "        mlp2 = self.mlp2(norm3, H, W)\n",
    "        \n",
    "        mx = add2 + mlp2\n",
    "        return mx\n",
    "    \n",
    "# Encoder\n",
    "class MiT(nn.Module):\n",
    "    def __init__(self, image_size, in_dim, key_dim, value_dim, layers, head_count=1, token_mlp='mix_skip'):\n",
    "        super().__init__()\n",
    "        patch_sizes = [7, 3, 3, 3]\n",
    "        strides = [4, 2, 2, 2]\n",
    "        padding_sizes = [3, 1, 1, 1]\n",
    "\n",
    "        \n",
    "        # patch_embed\n",
    "        # layers = [2, 2, 2, 2] dims = [64, 128, 320, 512]\n",
    "        self.patch_embed1 = OverlapPatchEmbeddings(image_size, patch_sizes[0], strides[0], padding_sizes[0], 3, in_dim[0])\n",
    "        self.patch_embed2 = OverlapPatchEmbeddings(image_size//4, patch_sizes[1], strides[1], padding_sizes[1],in_dim[0], in_dim[1])\n",
    "        self.patch_embed3 = OverlapPatchEmbeddings(image_size//8, patch_sizes[2], strides[2], padding_sizes[2],in_dim[1], in_dim[2])\n",
    "        # self.patch_embed4 = OverlapPatchEmbeddings(image_size//16, patch_sizes[3], strides[3], padding_sizes[3],in_dim[2], in_dim[3])\n",
    "        \n",
    "        # transformer encoder\n",
    "        self.block1 = nn.ModuleList([ \n",
    "            EfficientTransformerBlock(in_dim[0], key_dim[0], value_dim[0], head_count, token_mlp)\n",
    "        for _ in range(layers[0])])\n",
    "        self.norm1 = nn.LayerNorm(in_dim[0])\n",
    "\n",
    "        self.block2 = nn.ModuleList([\n",
    "            EfficientTransformerBlock(in_dim[1], key_dim[1], value_dim[1], head_count, token_mlp)\n",
    "        for _ in range(layers[1])])\n",
    "        self.norm2 = nn.LayerNorm(in_dim[1])\n",
    "\n",
    "        self.block3 = nn.ModuleList([\n",
    "            EfficientTransformerBlock(in_dim[2], key_dim[2], value_dim[2], head_count, token_mlp)\n",
    "        for _ in range(layers[2])])\n",
    "        self.norm3 = nn.LayerNorm(in_dim[2])\n",
    "\n",
    "        # self.block4 = nn.ModuleList([\n",
    "        #     EfficientTransformerBlock(in_dim[3], key_dim[3], value_dim[3], head_count, token_mlp)\n",
    "        # for _ in range(layers[3])])\n",
    "        # self.norm4 = nn.LayerNorm(in_dim[3])\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B = x.shape[0]\n",
    "        outs = []\n",
    "\n",
    "        # stage 1\n",
    "        x, H, W = self.patch_embed1(x)\n",
    "        for blk in self.block1:\n",
    "            x = blk(x, H, W)\n",
    "        x = self.norm1(x)\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        outs.append(x)\n",
    "\n",
    "        # stage 2\n",
    "        x, H, W = self.patch_embed2(x)\n",
    "        for blk in self.block2:\n",
    "            x = blk(x, H, W)\n",
    "        x = self.norm2(x)\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        outs.append(x)\n",
    "\n",
    "        # stage 3\n",
    "        x, H, W = self.patch_embed3(x)\n",
    "        for blk in self.block3:\n",
    "            x = blk(x, H, W)\n",
    "        x = self.norm3(x)\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        outs.append(x)\n",
    "\n",
    "        # stage 4\n",
    "        # x, H, W = self.patch_embed4(x)\n",
    "        # for blk in self.block4:\n",
    "        #     x = blk(x, H, W)\n",
    "        # x = self.norm4(x)\n",
    "        # x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        # outs.append(x)\n",
    "\n",
    "        return outs\n",
    "    \n",
    "# Decoder    \n",
    "class PatchExpand(nn.Module):\n",
    "    def __init__(self, input_resolution, dim, dim_scale=2, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.expand = nn.Linear(dim, 2*dim, bias=False) if dim_scale==2 else nn.Identity()\n",
    "        self.norm = norm_layer(dim // dim_scale)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        # print(\"x_shape-----\",x.shape)\n",
    "        H, W = self.input_resolution\n",
    "        x = self.expand(x)\n",
    "        \n",
    "        B, L, C = x.shape\n",
    "        # print(x.shape)\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "        x = rearrange(x, 'b h w (p1 p2 c)-> b (h p1) (w p2) c', p1=2, p2=2, c=C//4)\n",
    "        x = x.view(B,-1,C//4)\n",
    "        x= self.norm(x.clone())\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class FinalPatchExpand_X4(nn.Module):\n",
    "    def __init__(self, input_resolution, dim, dim_scale=4, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.dim_scale = dim_scale\n",
    "        self.expand = nn.Linear(dim, 16*dim, bias=False)\n",
    "        self.output_dim = dim \n",
    "        self.norm = norm_layer(self.output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        H, W = self.input_resolution\n",
    "        x = self.expand(x)\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "        x = rearrange(x, 'b h w (p1 p2 c)-> b (h p1) (w p2) c', p1=self.dim_scale, p2=self.dim_scale, c=C//(self.dim_scale**2))\n",
    "        x = x.view(B,-1,self.output_dim)\n",
    "        x= self.norm(x.clone())\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class MyDecoderLayer(nn.Module):\n",
    "    def __init__(self, input_size, in_out_chan, head_count, token_mlp_mode, n_class=9,\n",
    "                 norm_layer=nn.LayerNorm, is_last=False):\n",
    "        super().__init__()\n",
    "        dims = in_out_chan[0]\n",
    "        out_dim = in_out_chan[2]\n",
    "        key_dim = in_out_chan[2]\n",
    "        value_dim = in_out_chan[3]\n",
    "        x1_dim = in_out_chan[4]\n",
    "        self.inoutchannel = in_out_chan\n",
    "        if not is_last:\n",
    "            self.x1_linear = nn.Linear(x1_dim, out_dim)\n",
    "            #self.cross_attn = CrossAttentionBlock(dims, key_dim, value_dim, input_size[0], input_size[1], head_count, token_mlp_mode)\n",
    "            # project the shape back to half its size with a linear layer\n",
    "            self.concat_linear = nn.Linear(2*out_dim, out_dim)\n",
    "            self.norm_tr = nn.LayerNorm(out_dim)\n",
    "            # transformer decoder\n",
    "            self.layer_up = PatchExpand(input_resolution=input_size, dim=out_dim, dim_scale=2, norm_layer=norm_layer)\n",
    "            self.last_layer = None\n",
    "        else:\n",
    "            self.x1_linear = nn.Linear(x1_dim, out_dim)\n",
    "            self.cross_attn = CrossAttentionBlock(dims*2, key_dim, value_dim, input_size[0], input_size[1], head_count, token_mlp_mode)\n",
    "            self.concat_linear = nn.Linear(4*dims, out_dim)\n",
    "            # transformer decoder\n",
    "            self.norm_tr = nn.LayerNorm(out_dim)\n",
    "            self.layer_up = FinalPatchExpand_X4(input_resolution=input_size, dim=out_dim, dim_scale=4, norm_layer=norm_layer)\n",
    "            # self.last_layer = nn.Linear(out_dim, n_class)\n",
    "            self.last_layer = nn.Conv2d(out_dim, n_class,1)\n",
    "            # self.last_layer = None\n",
    "\n",
    "        #self.layer_former_1 = EfficientTransformerBlock(out_dim, key_dim, value_dim, head_count, token_mlp_mode)\n",
    "        #self.layer_former_2 = EfficientTransformerBlock(out_dim, key_dim, value_dim, head_count, token_mlp_mode)\n",
    "       \n",
    "\n",
    "        def init_weights(self): \n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.zeros_(m.bias)\n",
    "                elif isinstance(m, nn.LayerNorm):\n",
    "                    nn.init.ones_(m.weight)\n",
    "                    nn.init.zeros_(m.bias)\n",
    "                elif isinstance(m, nn.Conv2d):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.zeros_(m.bias)\n",
    "\n",
    "        init_weights(self)\n",
    "      \n",
    "    def forward(self, x1, x2=None):\n",
    "        if x2 is not None:# skip connection exist\n",
    "            b, h, w, c = x2.shape\n",
    "            x2 = x2.view(b, -1, c)\n",
    "            x1_expand = self.x1_linear(x1)\n",
    "            tran_layer_2 = self.norm_tr(self.concat_linear(torch.cat([x1_expand, x2], dim=2)))\n",
    "\n",
    "            if self.last_layer:\n",
    "                out = self.last_layer(self.layer_up(tran_layer_2).view(b, 4*h, 4*w, -1).permute(0,3,1,2)) \n",
    "            else:\n",
    "                out = self.layer_up(tran_layer_2)\n",
    "        else:\n",
    "            # if len(x1.shape)>3:\n",
    "            #     x1 = x1.permute(0,2,3,1)\n",
    "            #     b, h, w, c = x1.shape\n",
    "            #     x1 = x1.view(b, -1, c)\n",
    "            out = self.layer_up(x1)\n",
    "        return out\n",
    "    \n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels=3, reduction_ratio=1):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Linear(in_channels, in_channels // reduction_ratio)\n",
    "        self.fc2 = nn.Linear(in_channels // reduction_ratio, in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.conv = nn.Conv2d(3, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Squeeze\n",
    "        out = self.avg_pool(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "\n",
    "        # Excitation\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        # Reshape and scale\n",
    "        out = out.view(out.size(0), out.size(1), 1, 1)\n",
    "        out = x * out\n",
    "        out = self.conv(out)\n",
    "        return out\n",
    "\n",
    "class ContextBridge(nn.Module):\n",
    "    def __init__(self, x1_dim = 128, x2_dim = 320, dim = 512, token_mlp='mix'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn1 = EfficientAttention(in_channels = dim, key_channels=dim, value_channels=dim, head_count=1, flag_all = True)\n",
    "        self.attn2 = EfficientAttention(in_channels = dim, key_channels=dim, value_channels=dim, head_count=1, flag_all = True)\n",
    "        self.attn3 = EfficientAttention(in_channels = dim, key_channels=dim, value_channels=dim, head_count=1, flag_all = True)\n",
    "        \n",
    "        self.map1 = nn.Conv2d(x1_dim, dim, 1)\n",
    "        self.map2 = nn.Conv2d(x2_dim, dim, 1)\n",
    "        self.revmap1 = nn.Conv2d(dim, x1_dim, 1)\n",
    "        self.revmap2 = nn.Conv2d(dim, x2_dim, 1)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.norm3 = nn.LayerNorm(dim)\n",
    "        self.SE = SEBlock(in_channels=3, reduction_ratio=1)\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        x1 = self.map1(x1)\n",
    "        x2 = self.map2(x2)\n",
    "        \n",
    "        norm1 = x1\n",
    "        norm2 = x2\n",
    "        norm3 = x3\n",
    "        \n",
    "        attn1, v1 = self.attn1(norm1)\n",
    "        attn2, v2 = self.attn2(norm2)\n",
    "        attn3, v3 = self.attn3(norm3)\n",
    "        \n",
    "        context = self.SE(torch.cat([attn1.unsqueeze(1), attn2.unsqueeze(1), attn3.unsqueeze(1)], dim = 1))\n",
    "        context = context.squeeze(1)\n",
    "\n",
    "        attended_x1 = (context.transpose(1, 2) @ v1).reshape(x1.size())\n",
    "        attended_x2 = (context.transpose(1, 2) @ v2).reshape(x2.size())\n",
    "        attended_x3 = (context.transpose(1, 2) @ v3).reshape(x3.size())\n",
    "        \n",
    "        \n",
    "        attended_x1 = self.revmap1(attended_x1)\n",
    "        attended_x2 = self.revmap2(attended_x2)\n",
    "        return attended_x1, attended_x2, attended_x3\n",
    "\n",
    "    \n",
    "class ChannelEffFormer(nn.Module):\n",
    "    def __init__(self, num_classes=9, head_count=1, token_mlp_mode=\"mix_skip\"):\n",
    "        super().__init__()\n",
    "        self.context_bridge = ContextBridge(x1_dim = 128, x2_dim = 320, dim = 512, token_mlp='mix')\n",
    "        # Encoder\n",
    "        dims, key_dim, value_dim, layers = [[128, 320, 512], [128, 320, 512], [128, 320, 512], [2, 2, 2]]        \n",
    "        self.backbone = MiT(image_size=224, in_dim=dims, key_dim=key_dim, value_dim=value_dim, layers=layers,\n",
    "                            head_count=head_count, token_mlp=token_mlp_mode)\n",
    "        \n",
    "        # Decoder\n",
    "        d_base_feat_size = 7 #16 for 512 input size, and 7 for 224\n",
    "        in_out_chan = [[64, 128, 128, 128, 160],[320, 320, 320, 320, 256],[512, 512, 512, 512, 512]]  # [dim, out_dim, key_dim, value_dim, x2_dim]\n",
    "        # [[32, 64, 64, 64],[144, 128, 128, 128],[288, 320, 320, 320],[512, 512, 512, 512]]\n",
    "        # take out one layer\n",
    "        # self.decoder_3 = MyDecoderLayer((d_base_feat_size, d_base_feat_size), in_out_chan[3], head_count, \n",
    "        #                                 token_mlp_mode, n_class=num_classes)\n",
    "        self.decoder_2 = MyDecoderLayer((d_base_feat_size*2, d_base_feat_size*2), in_out_chan[2], head_count,\n",
    "                                        token_mlp_mode, n_class=num_classes)\n",
    "        self.decoder_1 = MyDecoderLayer((d_base_feat_size*4, d_base_feat_size*4), in_out_chan[1], head_count, \n",
    "                                        token_mlp_mode, n_class=num_classes) \n",
    "        self.decoder_0 = MyDecoderLayer((d_base_feat_size*8, d_base_feat_size*8), in_out_chan[0], head_count,\n",
    "                                         token_mlp_mode, n_class=num_classes, is_last=True)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #---------------Encoder-------------------------\n",
    "        if x.size()[1] == 1:\n",
    "            x = x.repeat(1,3,1,1)\n",
    "\n",
    "        output_enc = self.backbone(x)\n",
    "\n",
    "        b,c,_,_ = output_enc[2].shape\n",
    "\n",
    "        x1, x2, x3 = self.context_bridge(output_enc[0], output_enc[1], output_enc[2])\n",
    "        output_enc[0] = output_enc[0] + x1\n",
    "        output_enc[1] = output_enc[1] + x2\n",
    "        output_enc[2] = output_enc[2] + x3\n",
    "\n",
    "        #---------------Decoder-------------------------     \n",
    "        # tmp_3 = self.decoder_3(output_enc[3].permute(0,2,3,1).view(b,-1,c))\n",
    "        tmp_2 = self.decoder_2(output_enc[2].permute(0,2,3,1).view(b,-1,c))\n",
    "        tmp_1 = self.decoder_1(tmp_2, output_enc[1].permute(0,2,3,1))\n",
    "        tmp_0 = self.decoder_0(tmp_1, output_enc[0].permute(0,2,3,1))\n",
    "        #tmp_0 = torch.sigmoid(tmp_0)\n",
    "        return tmp_0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c60ff3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "model = ChannelEffFormer(num_classes=1, head_count=8, token_mlp_mode=\"mix_skip\")\n",
    "input = torch.rand(24, 3, 224, 224)\n",
    "print(model(input).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "authentic-healing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "ChannelEffFormer                                   [24, 1, 224, 224]         --\n",
       "├─MiT: 1-1                                         [24, 128, 56, 56]         --\n",
       "│    └─OverlapPatchEmbeddings: 2-1                 [24, 3136, 128]           --\n",
       "│    │    └─Conv2d: 3-1                            [24, 128, 56, 56]         18,944\n",
       "│    │    └─LayerNorm: 3-2                         [24, 3136, 128]           256\n",
       "│    └─ModuleList: 2-2                             --                        --\n",
       "│    │    └─EfficientTransformerBlock: 3-3         [24, 3136, 128]           346,624\n",
       "│    │    └─EfficientTransformerBlock: 3-4         [24, 3136, 128]           346,624\n",
       "│    └─LayerNorm: 2-3                              [24, 3136, 128]           256\n",
       "│    └─OverlapPatchEmbeddings: 2-4                 [24, 784, 320]            --\n",
       "│    │    └─Conv2d: 3-5                            [24, 320, 28, 28]         368,960\n",
       "│    │    └─LayerNorm: 3-6                         [24, 784, 320]            640\n",
       "│    └─ModuleList: 2-5                             --                        --\n",
       "│    │    └─EfficientTransformerBlock: 3-7         [24, 784, 320]            2,095,360\n",
       "│    │    └─EfficientTransformerBlock: 3-8         [24, 784, 320]            2,095,360\n",
       "│    └─LayerNorm: 2-6                              [24, 784, 320]            640\n",
       "│    └─OverlapPatchEmbeddings: 2-7                 [24, 196, 512]            --\n",
       "│    │    └─Conv2d: 3-9                            [24, 512, 14, 14]         1,475,072\n",
       "│    │    └─LayerNorm: 3-10                        [24, 196, 512]            1,024\n",
       "│    └─ModuleList: 2-8                             --                        --\n",
       "│    │    └─EfficientTransformerBlock: 3-11        [24, 196, 512]            5,318,656\n",
       "│    │    └─EfficientTransformerBlock: 3-12        [24, 196, 512]            5,318,656\n",
       "│    └─LayerNorm: 2-9                              [24, 196, 512]            1,024\n",
       "├─context_bridge: 1-2                              [24, 128, 56, 56]         3,072\n",
       "│    └─Conv2d: 2-10                                [24, 512, 56, 56]         66,048\n",
       "│    └─Conv2d: 2-11                                [24, 512, 28, 28]         164,352\n",
       "│    └─EfficientAttention: 2-12                    [24, 512, 512]            --\n",
       "│    │    └─Conv2d: 3-13                           [24, 512, 56, 56]         262,656\n",
       "│    │    └─Conv2d: 3-14                           [24, 512, 56, 56]         262,656\n",
       "│    │    └─Conv2d: 3-15                           [24, 512, 56, 56]         262,656\n",
       "│    │    └─Conv2d: 3-16                           [24, 512, 56, 56]         262,656\n",
       "│    └─EfficientAttention: 2-13                    [24, 512, 512]            --\n",
       "│    │    └─Conv2d: 3-17                           [24, 512, 28, 28]         262,656\n",
       "│    │    └─Conv2d: 3-18                           [24, 512, 28, 28]         262,656\n",
       "│    │    └─Conv2d: 3-19                           [24, 512, 28, 28]         262,656\n",
       "│    │    └─Conv2d: 3-20                           [24, 512, 28, 28]         262,656\n",
       "│    └─EfficientAttention: 2-14                    [24, 512, 512]            --\n",
       "│    │    └─Conv2d: 3-21                           [24, 512, 14, 14]         262,656\n",
       "│    │    └─Conv2d: 3-22                           [24, 512, 14, 14]         262,656\n",
       "│    │    └─Conv2d: 3-23                           [24, 512, 14, 14]         262,656\n",
       "│    │    └─Conv2d: 3-24                           [24, 512, 14, 14]         262,656\n",
       "│    └─SEBlock: 2-15                               [24, 1, 512, 512]         --\n",
       "│    │    └─AdaptiveAvgPool2d: 3-25                [24, 3, 1, 1]             --\n",
       "│    │    └─Linear: 3-26                           [24, 3]                   12\n",
       "│    │    └─ReLU: 3-27                             [24, 3]                   --\n",
       "│    │    └─Linear: 3-28                           [24, 3]                   12\n",
       "│    │    └─Sigmoid: 3-29                          [24, 3]                   --\n",
       "│    │    └─Conv2d: 3-30                           [24, 1, 512, 512]         4\n",
       "│    └─Conv2d: 2-16                                [24, 128, 56, 56]         65,664\n",
       "│    └─Conv2d: 2-17                                [24, 320, 28, 28]         164,160\n",
       "├─MyDecoderLayer: 1-3                              [24, 784, 256]            788,480\n",
       "│    └─PatchExpand: 2-18                           [24, 784, 256]            --\n",
       "│    │    └─Linear: 3-31                           [24, 196, 1024]           524,288\n",
       "│    │    └─LayerNorm: 3-32                        [24, 784, 256]            512\n",
       "├─MyDecoderLayer: 1-4                              [24, 3136, 160]           --\n",
       "│    └─Linear: 2-19                                [24, 784, 320]            82,240\n",
       "│    └─Linear: 2-20                                [24, 784, 320]            205,120\n",
       "│    └─LayerNorm: 2-21                             [24, 784, 320]            640\n",
       "│    └─PatchExpand: 2-22                           [24, 3136, 160]           --\n",
       "│    │    └─Linear: 3-33                           [24, 784, 640]            204,800\n",
       "│    │    └─LayerNorm: 3-34                        [24, 3136, 160]           320\n",
       "├─MyDecoderLayer: 1-5                              [24, 1, 224, 224]         305,408\n",
       "│    └─Linear: 2-23                                [24, 3136, 128]           20,608\n",
       "│    └─Linear: 2-24                                [24, 3136, 128]           32,896\n",
       "│    └─LayerNorm: 2-25                             [24, 3136, 128]           256\n",
       "│    └─FinalPatchExpand_X4: 2-26                   [24, 50176, 128]          --\n",
       "│    │    └─Linear: 3-35                           [24, 3136, 2048]          262,144\n",
       "│    │    └─LayerNorm: 3-36                        [24, 50176, 128]          256\n",
       "│    └─Conv2d: 2-27                                [24, 1, 224, 224]         129\n",
       "====================================================================================================\n",
       "Total params: 23,431,389\n",
       "Trainable params: 23,431,389\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 174.02\n",
       "====================================================================================================\n",
       "Input size (MB): 14.45\n",
       "Forward/backward pass size (MB): 17583.83\n",
       "Params size (MB): 89.09\n",
       "Estimated Total Size (MB): 17687.38\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model, input_size=input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-particle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
